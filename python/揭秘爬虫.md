[scrapy-network]: imgs/scrapy-network.png
[scrapy-request]: imgs/scrapy-request.png
[scrapy-response]: imgs/scrapy-response.png
[scrapy-develop-mode]: imgs/scrapy-develop-mode.png
[scrapy-web-model]: imgs/scrapy-web-model.png
[scrapy-web-dom]: imgs/scrapy-web-dom.png
[scrapy-js-objects]: imgs/scrapy-js-objects.png

### 揭秘爬虫

爬虫，其实就是访问别人的网站，把别人网站上的数据和内容采集下来。通常，爬虫会将数据和内容保存到服务器数据库，然后对外提供二次服务。

爬虫本质是一个Web页面浏览程序，用以访问、分析、下载目标网页的数据。

举例
1. 将某个页面上的数据生成表格保存本地。
2. 将某个页面上的所有链接保存下来。
3. 将某个页面上的所有图片下载到本地。
4. 将某个站点下的所有页面的所有图片下载到本地。
5. 将某个站点拷贝一份。

复杂的爬虫的访问链是树型的：

```
              www.demo.com/index.html
				 /       |        \
www.demo.com/a.html  .../b.html    .../c.html
	                  /        \
		       .../d.html   .../e.html
			         |          |
					end		.../f.html
					            |
							   end
```
最顶层的页面下，包含多个链接；分析完顶层后，爬虫需要对这几个链接分别进行访问，继而分析下一层。依此类推，某个页面没有链接，表示到达某个叶子节点。当所有链路都到达叶子节点时，则表示这个站点的主页被爬完。

一个站点，除了从主页跳转的链接外，还可能有很多单独的页面地址。这些单独的页面地址，只能靠穷举了。

由于爬虫访问过程是树型，且是不断发现和分析的过程，所以算法有：

* 广度优先搜索，分析完一层后，再分析下一层。
* 深度优先搜索，发现一个节点就分析一个节点，直到其中一个叶子。
* 最佳优先搜索，根据URL匹配原则预判相关性然后决定是否分析。

#### 第一个爬虫

```python
# -*- coding:utf-8 -*-
# @author jokinkuang

import urllib2
import bs4
import sys

if len(sys.argv) < 2:
    print "Usage: "+sys.argv[0]+" keyword"
    exit(0)

key = sys.argv[1]
print "Searching:"+key

url = "https://baike.baidu.com/item/"+key
ua_header = {"User-Agent":"Mozzila/5.0(compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;"}
request = urllib2.Request(url,headers=ua_header)

response = urllib2.urlopen(request)
html = response.read().decode('utf-8')
soup = bs4.BeautifulSoup(html,"lxml")

result = "Not found baike for:"+key
try:
    result = soup.find_all("div", class_="lemma-summary")[0].text
except:
    pass
print result
```

运行结果：

```shell
$ python2.7 baike-search.py
Usage: baike-search.py keyword

$ python2.7 baike-search.py 呵呵哒
Searching:呵呵哒

一种流行的网络用语，在“呵呵”的基础上显得更萌，也更高冷。出自《暴走漫画》第三季，并且在此之前已有许多人开始接受这一新型词语。呵呵哒作为年轻人来讲，已经成为了不可或缺的网络聊天语。假如有人呵呵你，你可能会感到生气。如果对方发出“呵呵哒”，也许同样在调侃你，口气稍带轻蔑，但这种语气比前者弱了很多，会使人认为在开玩笑。

$ python2.7 baike-search.py 666
Searching:666

666是一个汉语词汇，谐音为牛牛牛，溜溜溜有一种本土化的意思，用来形容某人或某物很厉害很牛、令人折服（大多是指游戏玩的好）。而在西方，666指魔鬼，撒旦和灵数，是不吉利的象征。随着时代的发展，各类游戏的节奏也越来越快。666这样的语言用的越来越多。在江苏破获的网络涉毒案中，犯罪嫌疑人用666暗指吸毒，意思是让你溜。当人们表现出超常的能力时也会用来感叹。
```

总结：

1. 组装目标地址
2. 获取目标地址内容
3. 分析得到结果

#### Web模型

由于爬虫是抓取Web站点数据，所以，在学习爬虫前，需要先了解Web模型。

![scrapy-web-model][scrapy-web-model]

如图。`浏览器`访问`index.html`然后登录并获取某些数据。这个过程始终发生在`index.html`页面，从加载到结束经历了上述的请求和回包。

1. 浏览器加载获取任意文件，都会发送Get请求到服务器，并有自己的Headers，Cookies和Params。
2. 浏览器返回任意文件，比如html/js/css/fonts都是一个应答包，并有自己的Headers，Cookies和Body。（**只不过文件的Body是字节流，html/js/css文件的Body是字符串，api的Body是json或xml**）
3. *任意请求，都有Headers／Cookies／Params*
4. *任意应答，都有Headers／Cookies／Body*

爬虫主要是获取服务器应答的数据，所以更关注应答包。*新请求中的Params参数，大部分来自旧的应答包，尤其是来自Cookies*。由于任意请求都会产生应答包，所以Params参数可能来自任意一次应答包！但通常是：

* Get js父容器页面时的应答包（js都是寄宿在html页面）。
* Get/Post api的应答包。
* 应答包除了要关注Body，还要关注Headers和Cookies。

#### 浏览器，Dom模型和Javascript环境

浏览器，Dom模型和javascript运行环境并没有直接联系，三者相互独立。如下图。

![scrapy-js-objects][scrapy-js-objects]

* Javascript环境内置的对象，如`Array`,`Date`,`Number`；
* 浏览器在Javascript环境下的对象，如`Window`,`Screen`；
* Html Dom在Javascript环境下的对象，如`Document`,`Element`,`Attribute`；
* Html标签在Javascript环境下的对象，如`Body`,`Button`,`Image`。

Html Dom对象和Html 标签对象是有区别的。
Dom模型将所有html标签都抽象成`Element`，所有属性抽象成`Attribute`。没有细分为`Button`,`Image`等对象。
通常，只有浏览器下才会有Html标签对象，其它html解析器通常只遵循Dom模型接口。

只包含Javascript内置对象的环境，叫**纯javascript环境**。扩展了浏览器对象，Html Dom对象的javascript环境，叫**浏览器端javascript环境**，其它的扩展了服务端对象的javascript环境，则叫**服务端javascript环境**。

> 插一句，浏览器端扩展`浏览器`对象和扩展`Dom`对象，其实类似于Javascript(浏览器)，Javascript(Html Text)这样，通过当前浏览器，当前访问的Html页面来初始化`Window`,`Screen`对象和`Document`,`Element`对象。

Dom模型，它只是定义了一套Html文档操作接口，按照这套接口，可以访问和修改Html文档。Dom约定的是标准对象，无论它使用什么语言实现，它都要有标准接口。说白了就是，如果你定义了一个html dom解析器，那么它的输出应当是标准的Dom对象，有Dom模型约定的所有接口。这套约定的提出，主要是解决跨平台兼容问题。

#### Python环境和浏览器环境的区别
Python环境和浏览器环境的区别是非常大的，习惯了浏览器环境后可能会无法理解其它环境下页面处理的一些问题。比如：

* html解析
* 如何便捷的动态修改html的内容
* 如何使用selector便捷查找html里面的值
* 如何执行html里内嵌的javascript
* 如何执行html导入的javascript文件

![scrapy-web-dom][scrapy-web-dom]

如图，浏览器环境下，浏览器得到html文本后，通过html解析器得到Javascript Dom对象（意思是Dom模型中的对象使用Javascript语言实现）。浏览器得到js文本后，加载到js解析器，两者由于都是Javascript对象，所以可以相互访问，所以浏览器环境下，Javascript环境有`Window`,`Document`等对象。这些对象其实是浏览器负责扩展加载到javascript环境的。

Python环境下，如果要模拟`浏览器端javascript环境`，需要提供一个Python javascript运行环境，且需要支持初始化`浏览器对象`和`Dom对象`，这样才能访问`Window`，`Document`等对象。

在python环境下，还没有好的`浏览器端javascript`运行环境。据说`PyExecJS`可以，还没用。
目前常见的是`pyv8`的库，`pyv8`是使用Python实现的一个`纯javascript运行环境`。（如上，它只提供javascript标准中的对象，并没有提供`Window`,`Screen`等浏览器对象，和`Document`,`Element`等Dom对象）。

html解析器有多种，有独立的，有“融合”的。“融合”的是指，html解析器解析出来的Dom对象，还能被当前环境使用，比如`浏览器端javascript环境`就是融合的html解析器，它具备解析Html的能力，且解析出来的对象可以被javascript环境使用。

Python下独立的Html解析器，有`BeautifulSoup`。

```python
# -*- coding:utf-8 -*-
import bs4
html="""<html>
<body>
   <div id="val">Hello World!</div>
</body>
</html>"""
soup = bs4.BeautifulSoup(html,"lxml")
element = soup.find(id="val")
print type(element)
print element.text

输出
<class 'bs4.element.Tag'>
Hello World!
```

如上，得到html文本后，通过Python Html解析器可以得到Python Dom对象（意思是Dom模型中的对象使用Python语言实现）。然后可以通过Dom对象访问和修改Html文档。

在Python环境下，如果Html使用`BeautifulSoup`加载，js通过`pyv8引擎`加载，两者并不能形成`浏览器端Javascript环境`。因为Html解析器是一个独立的库，pyv8也是独立的库，两者并不能“融合”。

要pyv8的`纯javascript环境`扩展为`浏览器端javascript环境`，需要pyv8扩展实现Html解析器功能。这就是pyv8 demo中`browser.py`和`w3c.py`这两个Python文件的功能，实现了浏览器对象，和Html到Dom对象的转换。

但由于浏览器对象和Dom对象比较复杂，`browser.py`和`w3c.py`并没有完全实现标准上的所有接口。所以这导致了兼容性问题。原始的`javascript文件`加载到这个pyv8搭建的`浏览器端javascript运行环境`，会出现部分方法找不到的异常。

上面的问题，总结一下就是：

* html解析，使用任意的Html解析器即可。
* 先使用html解析器将html转化为Dom对象，就可以便捷的动态修改html的内容
* 同上，将html转化为Dom对象后，就可以使用selector便捷查找html里面的值
* Python javascript环境并不是浏览器环境，不会执行html里面的脚本，所以需要将内嵌的javascript抽取出来执行。
* html引用的javascript文件，如果涉及到`window`或`document`对象，需要使用融合了Dom模型的javascript运行环境。

最后，无论是哪种语言，要实现一个`浏览器端javascript环境`，需要提供

* 浏览器对象
* Html Dom解析器
* Html Dom对象
* 标准的Javascript运行环境（纯javascript环境）

#### 开始编写爬虫
因为爬虫本质是web访问程序，所以任何一种编程语言都可以作为编写爬虫的语言。

相对来说，使用python会比较容易，主要是因为python简单易用，本身有提供比较高级的封装，还有很多专为网页分析而生的第三方库，在网页处理方面有其它语言无法比拟的优势。（其它就不多说了）

#### 第一步，分析目标网站

**没有登录验证**
如果目标网站不需要登录即可以访问，那么相对轻松。因为不需要登录，也就不需要管理Cookie/Session，只需要按照常规的Get／Post请求即可。

**需要登录验证**
如果目标网站需要先登录，那么相对困难。因为需要登录意味着要自己管理Cookie/Session，在发送Get／Post请求时需要带上Cookie/Session才能进行访问。

分析过程，可以借助浏览器的**开发者模式**。如图，通过浏览器工具可以调出。

![scrapy-network][scrapy-network]

主要需要看`Network`标签，勾选`Preserve Log`，然后可以选择性的选择`Filter`过滤。接着在浏览器执行操作，左侧列表就会有一系列网络请求包。从中可以看到，一个Http请求的Request和服务器的Response。如下。

![scrapy-request][scrapy-request]

`Request`中，最重要的是`Cookie`，其次可能会有一些自定义的字段，比如上图中的`X-token`；通常`User-Agent`和`Referer`也会被后台识别。

![scrapy-response][scrapy-response]

`Response`中，最重要的是`Set-Cookie`，这个值意味着，返回的这些Cookie要放在下一次请求中，否则服务器会拒绝访问。（如果没有Set-Cookie，则可以一直使用旧的Cookie）

其次，还可能会有`Location`等等其它属性，具体应答具体分析。

`Request`和`Response`之间是相互作用的，也可能相互验证。当服务器发觉当前Request与上一次的Response某些“关键字段”丢失的时候，很可能会拒绝访问。简而言之，Request和Response就像一环接一环的链条。服务器可能会检查链条间的顺序是否正确，一旦错误，服务器都可能会拒绝访问。

```
 Request(A) - Response(B+C) - Request(B+C) - Response(B+C+D) - request(B+C+D) ...
 // 链条间都有某种规律，这种规律就是服务器与客户端间的约定。
```

**采集的是静态内容**

静态内容，指的是不需要执行脚本，就存在于页面源码中的内容。比如，jsp／php／html等页面源码中的内容。比如：

`www.xxx.com/xxx.html`
`www.xxx.com/xxx.jsp`
`www.xxx.com/xxx.php`

```html
<html>
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta name="referrer" content="always" />
<meta name="description" content="test，意为测试...">
<meta name="token" content="3923293">
<meta name="image" content="https://bkssl.bdimg.com/cms/static/baike.png">
<script type="text/javascript" src="https://www.xxx.com/init.js?st=17623"></script>
</head>
... ...
</html>
```

静态页面返回的是html格式的内容，所以要获取静态内容，需要解析html标签。

有不少关键的信息，存储在html头部的`meta`里面，用于下次发起请求的关键字段。比如上面例子中的`token`，在以后与服务器的通信中，都会用到。所以需要解析html的标签，获取token对应的值。

> *开发者模式* 下Sources目录下的html文件是静态页面。如下图。

**采集的是动态内容**

目前大部分网页，都是**静态网页+动态内容**的模式。静态的页面内容，配合脚本生成的动态内容，最终生成浏览器端看到的页面。比如：

`www.xxx.com/index.html`
+ `www.xxx.com/api/items`
+ `www.xxx.com/api/ads`

```html
// api/items
{"errno":0,"errmsg":"","data":{ ... }}

// api/ads
{"errno":0,"errmsg":"","data":{ ... }}
```
前者是html框架，后者得到的是json/xml数据，然后通过js动态添加到html框架中。
所以，如果无法运行javascript，就不能得到最终渲染出来的页面：

如果要抓取渲染后的页面内容，需要有支持javascript运行的环境，或者拿到数据后自行组装成目标内容。
如果要抓取的仅仅是web api中的数据，直接抓取api即可，无需关心页面的渲染。

> *开发者模式* 下Element目录下的html内容是渲染后的页面，即经过js加工后的页面。如下图。

![scrapy-develop-mode][scrapy-develop-mode]

#### 第二步，模拟请求

爬虫的关键点在于，模拟Web请求，让服务器“认为”当前请求是合法的，继而得到`Response`并从中获取数据。

不需要登录的站点，模拟的难度低，服务器通常不会在收到`Request`时进行过多的验证。模拟的请求可以很轻易的“骗过”服务端，得到`Response`。

而需要登录的站点，验证可能无处不在。服务器收到`Request`时，除了会验证当前`Request`具备的数据是否合法，还可能会验证上一次`Response`中的关键数据是否健在。一旦发现某些字段异常，服务端将会拒绝访问。

一个简单的模拟请求的例子。
```python
import urllib2
url = "http://www.baidu.com"
ua_header = {"User-Agent":"Mozzila/5.0(compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;"}
request = urllib2.Request(url,headers=ua_header)
response = urllib2.urlopen(request)
html = response.read()
print html
```

大致就是，创建一个Get或Post请求，然后设置对应的`Headers`,`Cookies`,`Referer`等等，并附带上额外的参数，然后发到服务端。如上面Request的图，尽量和分析时得到的Request保持一致。这个过程就是“模拟”请求。

另外 headers 的一些属性，需要特别注意：

* Referer : 有些服务器会识别是不是来自自己的站点，如果不是，可能会拒绝，用于防止外链，防盗链。
* User-Agent : 有些服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求
* Content-Type : 在使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析。
 	application/xml ： 在 XML RPC，如 RESTful/SOAP 调用时使用
	application/json ： 在 JSON RPC 调用时使用
	application/x-www-form-urlencoded ： 浏览器提交 Web 表单时使用
	在使用服务器提供的 RESTful 或 SOAP 服务时， Content-Type 设置错误会导致服务器拒绝服务

总而言之，如果服务器拒绝访问，就说明“模拟”的请求不够接近。要么缺失了字段，要么是字段里的数据不合法。
模拟请求是爬虫最关键的一步，尽量接近浏览器里的Request是成功的关键。

**变化的参数值**

每一次登录或请求，参数的字段不变，参数值可能需要改变。常见的有：

* 时间戳
* 随机值
* cookie
* 其它字段

这些可变的参数值，可能会来自：

* 首次登录的`Response`，比如`uuid`、`cookie`。
* 首次登录后`主页html`中的`meta`，比如`token`。
* 上一次`Response`中带回的字段。

**Cookie的管理**

```shell
Client -> name+pass+login -> Server
Client <- cookie/token/session <- Server
Client -> Request + cookie/token/session -> Server
Client <- Response + new cookie/token/session <- Server
...
```
如上，客户端登录时，通过账号密码登录站点。通常，这个过程不需要提供任何cookie。
当登录成功后，服务端就返回当前用户的cookie/token/session给客户端，在下一次请求时，客户端需要带上这些关键信息。
下一次请求，客户端带着自己的登录信息，加上自己的Request，发送给服务端；服务端返回数据的同时，可能带上新的cookie/token/session给客户端，在下一次请求时，客户端就需要带上新的关键信息。
依此类推，一环接一环。

整个过程，cookie的管理是比较繁琐的。幸运的是，python大部分提供web服务的库，都包含了cookie的管理。也即是说，这个过程，开发者只需要关心Request中的其它自定义字段和数据即可。比如，上面提到的`token`,`X-token`等等。

**加密数据的模拟**

“模拟”请求时，最困难的是加密数据的模拟，因为你需要突破加密算法，否则就得不到正确的值，无法得到正确的值，就无法通过服务器的验证。

例如：QQ的登录。
QQ的登录过程，会对密码进行加密，然后Post到服务端。加密算法位于混淆的javascript中，要破解几乎不可能。

所以一个通用的做法是，模拟整个javascript环境，将包含登录的那个js下载下来，然后找到入口，调用得到加密后的串。相当于在浏览器里点击登录按钮后执行加密过程一样。

#### 第三步，数据处理

只要Response成功返回，一切就好办。
如果返回的是`html`，使用html解析器就能得到想要的内容；如果返回的是`json`等数据，无需解析就能得到。
得到数据后，可以进一步加工：

* 持久化
* 报表
* 邮件

大部分爬虫都将采集到的数据存储到数据库。这一步没什么好说的。
